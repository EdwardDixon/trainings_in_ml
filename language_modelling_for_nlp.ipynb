{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_modelling_for_nlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdwardDixon/trainings_in_ml/blob/master/language_modelling_for_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "k4VDYKT_6CZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning for Text\n",
        "*Using a pre-trained language model to play with NLP*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First, let's make sure you have a GPU attached to your Colab notebook.  When you run the next sell, the output should tell you that you've got a Tesla K80 attached to this notebook.  If not, you'll need to go to the _\"Edit\"_ menu, then choose \"Notebook Settings\" and select \"GPU\" from the \"Hardware Accelerator\" dropdown."
      ]
    },
    {
      "metadata": {
        "id": "dZx25HM258oY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAJlEY9f7GBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installing Pre-requisites\n",
        "Next, we need to install the libraries we will rely on.  The `pytorch-pretrained-bert` library comes from [the nice people at Hugging Face](https://huggingface.co/) and includes pre-trained PyTorch ports of the latest language models and some nice helper code to get your text ready to feed into them.  Notice the `!` prefix - these are commands we are running in the shell, not Python code. The `SpaCy` library is becoming something of an industry standard for NLP."
      ]
    },
    {
      "metadata": {
        "id": "24lZ48Y-YgeF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install spacy ftfy==4.4.3\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U32ePVfK8BW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the text\n",
        "We need to do a little work before we can get our text into the model.  [Bert is a word-piece model](https://github.com/google-research/bert), which means that it operates on words and parts of words.  The version that we will be using, `bert-base-uncased`, has 12 layers, 110M parameters, and a vocabulary of about 30K words.  Other Bert variants have double the number, distinguish between lower and upper case, or have been trained ion multi-lingual datasets."
      ]
    },
    {
      "metadata": {
        "id": "yFnvt72vjtM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenized input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = 8\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
        "\n",
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGMeAiusmFU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text to Vectors\n",
        "Now that our text is in tensors, we can push it into the model.  This will get us hidden states that we can use for our NLP tasks..."
      ]
    },
    {
      "metadata": {
        "id": "nr1Xy3oqmItK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
        "assert len(encoded_layers) == 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1IwnBJ2xA5dL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khMZyfHi5QBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see if the model can predict the missing token..."
      ]
    },
    {
      "metadata": {
        "id": "L9ScxBYD5OWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "print(\"Prediction has shape \" + str(predictions.shape) + \"\\n\")\n",
        "    \n",
        "# confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "print(\"Predicted token = \" + predicted_token)\n",
        "\n",
        "# ...and the least likely token?\n",
        "not_predicted_index = torch.argmin(predictions[0, masked_index]).item()\n",
        "not_predicted_token = tokenizer.convert_ids_to_tokens([not_predicted_index])[0]\n",
        "print(\"Least likely value: \" + not_predicted_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYzE_T5rQUV7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Near misses?\n",
        "What about the words that didn't quite make it?  Our prediction vector is a softmax over the entire vocabularly, so we can take a look at the next-highest-scoring words.  I've taken the top 5.  Note we need to bring the results vector back to the CPU from the GPU's memory."
      ]
    },
    {
      "metadata": {
        "id": "Us3HJPPJCdfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Top 5 guesses?\n",
        "from torch import topk\n",
        "token_indices = topk(predictions[0, masked_index],5)[1]\n",
        "print(token_indices)\n",
        "topk_tokens = tokenizer.convert_ids_to_tokens(token_indices.cpu().numpy())\n",
        "print(topk_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHBgtV7CjA_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Time for classification!\n",
        "Let's start by getting a dataset with two classes.  We can use [this repository I created for my chaper in Online Harassment](https://github.com/EdwardDixon/Automation-and-Harassment-Detection)."
      ]
    },
    {
      "metadata": {
        "id": "iRTuGyaOjJB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a677c6e1-5e0b-471f-cb13-5508b013dc18"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EdwardDixon/Automation-and-Harassment-Detection"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Automation-and-Harassment-Detection'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Total 39 (delta 0), reused 0 (delta 0), pack-reused 39\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7XzLpVTlOLf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can load the data and view a sample."
      ]
    },
    {
      "metadata": {
        "id": "xYnKyyJTlStV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "30663412-e69a-41ea-b028-0ec1ac8ff624"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"Automation-and-Harassment-Detection/data/attack_train.csv\")\n",
        "df_train.tail()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rev_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>year</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>ns</th>\n",
              "      <th>sample</th>\n",
              "      <th>split</th>\n",
              "      <th>attack</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>69521</th>\n",
              "      <td>699756185</td>\n",
              "      <td>\"   The lead itself is original research. Wher...</td>\n",
              "      <td>2016</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69522</th>\n",
              "      <td>699813325</td>\n",
              "      <td>\" ::I'm talking about you making unjustified m...</td>\n",
              "      <td>2016</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>0.157895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69523</th>\n",
              "      <td>699848324</td>\n",
              "      <td>\"   These sources don't exactly exude a sense ...</td>\n",
              "      <td>2016</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69524</th>\n",
              "      <td>699857133</td>\n",
              "      <td>:The way you're trying to describe it in this...</td>\n",
              "      <td>2016</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69525</th>\n",
              "      <td>699897151</td>\n",
              "      <td>Alternate option=== Is there perhaps enough ne...</td>\n",
              "      <td>2016</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>blocked</td>\n",
              "      <td>train</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          rev_id                                            comment  year  \\\n",
              "69521  699756185  \"   The lead itself is original research. Wher...  2016   \n",
              "69522  699813325  \" ::I'm talking about you making unjustified m...  2016   \n",
              "69523  699848324  \"   These sources don't exactly exude a sense ...  2016   \n",
              "69524  699857133   :The way you're trying to describe it in this...  2016   \n",
              "69525  699897151  Alternate option=== Is there perhaps enough ne...  2016   \n",
              "\n",
              "       logged_in       ns   sample  split    attack  \n",
              "69521       True  article  blocked  train  0.100000  \n",
              "69522       True  article  blocked  train  0.157895  \n",
              "69523       True  article  blocked  train  0.111111  \n",
              "69524       True  article  blocked  train  0.000000  \n",
              "69525       True  article  blocked  train  0.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}